# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /datamodule: ts_datamodule.yaml
  - override /model: bilstm.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["synthetic", "bilstm"]

seed: 1234

trainer:
  min_epochs: 50
  max_epochs: 300
  accelerator: gpu

model:
  optimizer:
    _target_: torch.optim.RAdam
    # lr: 1e-3
    # weight_decay: 0.001
  # net:
  #   dropout: 0.1
  #   d_model: 64
  #   nhead: 4
  #   dim_feedforward: 256
  #   mlp_dim: 128
  #   nlayers: 4

datamodule:
  # data_dir: ${paths.data_dir}/dataset_synthetic
  # data_dir: ${paths.data_dir}/dataset_synthetic_snr20
  data_dir: ${paths.data_dir}/dataset_synthetic_nmi
  feature_name: signal
  # target_name: target_sum
  target_name: target
  id_name: noun_id
  train_batch_size: 64
  val_batch_size: 1
