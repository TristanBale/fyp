<div align="center">

# Time-series classification & post-hoc interpretability & robustness evaluation pipeline

[![python](https://img.shields.io/badge/-Python_3.9-blue?logo=python&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![pytorch](https://img.shields.io/badge/PyTorch_1.13.1-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![lightning](https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)

</div>


## Installation

#### Pip

```bash
# clone project
git clone https://github.com/YourGithubName/your-repo-name
cd your-repo-name

# [OPTIONAL] create conda environment
virtualenv venv --python=python3.9
source venv/bin/activate

# install pytorch according to instructions
# https://pytorch.org/get-started/

# install requirements
pip install -r requirements.txt
```

## Project structure

```
├── configs                   <- Hydra configs
│   ├── callbacks                <- Callbacks configs
│   ├── datamodule               <- Datamodule configs
│   ├── debug                    <- Debugging configs
│   ├── experiment               <- Experiment configs
│   ├── extras                   <- Extra utilities configs
│   ├── hparams_search           <- Hyperparameter search configs
│   ├── hydra                    <- Hydra configs
│   ├── local                    <- Local configs
│   ├── logger                   <- Logger configs
│   ├── model                    <- Model configs
│   ├── paths                    <- Project paths configs
│   ├── trainer                  <- Trainer configs
│   │
│   ├── eval.yaml             <- Main config for evaluation
│   └── train.yaml            <- Main config for training
│
├── data                   <- Project data
│
├── logs                   <- Logs generated by hydra and lightning loggers
│
├── notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),
│                             the creator's initials, and a short `-` delimited description,
│                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.
│
├── scripts                <- Shell scripts
│
├── src                    <- Source code
│   ├── datamodules              <- Datamodule scripts
│   ├── models                   <- Model scripts
│   ├── utils                    <- Utility scripts
│   │
│   ├── eval.py                  <- Run evaluation
│   └── train.py                 <- Run training
│
├── .gitignore                <- List of files ignored by git
├── .project-root             <- File for inferring the position of project root directory
├── requirements.txt          <- File for installing python dependencies
└── README.md
```

## How to run

Train model with default configuration

```bash
# train on CPU
python src/train.py trainer=cpu

# train on GPU
python src/train.py trainer=gpu
```

### [Recommended]

### 1. Train a model for time series classification
Train model with chosen experiment configuration from [configs/experiment/](configs/experiment/)

```bash
python src/train.py experiment=earthquake.yaml logger=tensorboard hparams_search=optuna.yaml
```
The trained results will be automatically saved to dir `logs/train/runs/<yy-mm-dd_time>/`

### 2. Generate interpretability for trained model and evaluate interpretability results
```bash
python src/interpret.py 
```
Note: Please indicate checkpoint path of trained model in [configs/eval_interpret.yaml](configs/eval_interpret.yaml)
The evaluation results will be automatically saved to dir `logs/interpret/runs/<yy-mm-dd_time>/`

### 3. Calculate Xpower metric for interpretability

Please copy the file `postprocess.ipynb` into the dir `logs/interpret/runs/<yy-mm-dd_time>/` to calculate metric Xpower for the interpretability already generated in Step 2.


## How to generate synthetic dataset
1. Specify the configurations for synthetic dataset generation in `configs/synthetic_data.yaml`, e.g., the number of total instances, features, time steps. Please indicate the `save_name`, which will be used as the file name for saving new dataset.

2. Execute `src/synthetic.py' to generate the synthetic dataset
```bash
python src/synthetic.py
```
The new generated synthetic dataset will be automatically saved to dir `data/save_name`.
<!-- You can override any parameter from command line like this

```bash
python src/train.py trainer.max_epochs=20 data.batch_size=64
``` -->
